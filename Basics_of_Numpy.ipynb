{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efce1daa-467d-4c34-a3c2-a70b7fb9d2e0",
   "metadata": {},
   "source": [
    "## Basics of Python for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c3c55-b92f-4734-aebd-9da490ac30ee",
   "metadata": {},
   "source": [
    "- This exercise gives you a brief introduction to Python. Even if you've used Python before, this will help familiarize you with the functions we'll need.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ef122-5430-4c51-8c3e-8c8b335adbf0",
   "metadata": {},
   "source": [
    "- Even if you know basics of python, this blog will help you to get to a place where you will get familiar with how to use python for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3b9f0-5719-434e-afad-e26286dd3e6b",
   "metadata": {},
   "source": [
    "- As we are developing the skill for ML with python, we need to make sure that we write an efficient code for model development. In ML, efficient code can be considered as the code without for loops.\n",
    "---\n",
    "#### Why?\n",
    "ML is the process of learning things, which takes multiple trials. We cant learn a topic just by reading once there is a process of revision, which is a iterative process. If you know coding, you will get to know that iterative process needs for loop as their syntax. But for loop is not efficient in this context as it consumes lot of time. But there are some situations like epochs where you need for loop explicitly cant do anything there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137d146-0bff-4ed1-803a-bb22a77379ff",
   "metadata": {},
   "source": [
    "- After reading and trying the code in this blog, you will be able to use iPython notebooks and able to use numpy functions and matrix/vector operations and also understand the concept of \"broadcasting\". \"Broadcasting\" is one of the important concept in python that will be frequently used in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d4606-4126-4fe5-9482-e13a5bcf55f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World!\n"
     ]
    }
   ],
   "source": [
    "test = \"Hello World!\"\n",
    "print(\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59bdfd-6b20-4e3b-874d-6c5fff492d65",
   "metadata": {},
   "source": [
    "- Run your cells using SHIFT + ENTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5b907-f784-4d07-8ae2-8f96e894dbbe",
   "metadata": {},
   "source": [
    "## Building basic functions with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc8123-f589-4fe2-bed5-ebd10182921e",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Numpy is one of the main package in python for scientific computing. It is maintained by large community that is www.numpy.org.\n",
    "- In this blog, we will learn several numpy functions like 'np.exp', 'np.log', and 'np.reshape'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d08c19-564c-41d4-a7b5-ebc4a3ec9228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_sigmoid(1) = 0.7310585786300049\n"
     ]
    }
   ],
   "source": [
    "# Lets develop the sigmoid function using numpy\n",
    "## We use np.exp() in this function.But if i put numpy aside as I learnt python I know one of the function in math module that is math.exp().\n",
    "### Okay then first lets try with math.exp()\n",
    "import math\n",
    "def basic_sigmoid(x):\n",
    "    s = 1/(1+math.exp(-x))\n",
    "    return s\n",
    "\n",
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f21e28-04e0-471b-8e81-2a7507649372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_sigmoid(1) = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mbasic_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasic_sigmoid\u001b[39m(x):\n\u001b[1;32m----> 5\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "## Okay math.exp() is working good with real number as the input\n",
    "### What if input is matrix which is widely used in deep learning?\n",
    "## Lets see\n",
    "def basic_sigmoid(x):\n",
    "    s = 1/(1+math.exp(-x))\n",
    "    return s\n",
    "\n",
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ca8deb-a2c1-48e2-a96b-cf74cf170da2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "# You can see an error saying input is list\n",
    "## Now lets implement the same function with numpy\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3521b8-182a-4542-b74b-42b5084f47b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also create a new cell in the notebook and write `np.exp?` (for example) to get quick access to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de984ccf-b75f-4cb7-840c-a58c82c51b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19661193324148185"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets try to impolement Sigmoid Gradient\n",
    "### We know that gradients are usefull to optimize the loss function using backpropagation\n",
    "## d(sigmoid) = sigmoid * (1 - sigmoid)\n",
    "def sigmoid_derivative(x):\n",
    "    ds = sigmoid(x)\n",
    "    return ds * (1 - ds)\n",
    "\n",
    "sigmoid_derivative(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ac6474-72dd-4974-aef6-40b0de0db144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.10499359, 0.04517666])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a017-32bf-455c-8205-0f8d50ea7e3b",
   "metadata": {},
   "source": [
    "- Two common functions used in deep learning are np.shape() and np.reshape()\n",
    "x.shape() -> tells the shape of the matrix/vector x.\n",
    "x.reshape() -> can change the shape of the matrix/vector x\n",
    "\n",
    "---\n",
    "\n",
    "- Lets take an example, in computer sciene image is represented as 3d array of shape $(length, height, depth = 3)$. When you read this image in the python, the algorithm will convert the shape of the image x into $(length*heght*3,1)$. In other words you will convert the 3D image into 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9258d88-2a4a-4a7f-9a3e-b96805ebefb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "t_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1a0862-66df-491b-b1a3-85a499556c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_image[0]: [[0.67826139 0.29380381]\n",
      " [0.90714982 0.52835647]\n",
      " [0.4215251  0.45017551]]\n",
      "t_image[1]: [[0.92814219 0.96677647]\n",
      " [0.85304703 0.52351845]\n",
      " [0.19981397 0.27417313]]\n",
      "t_image[2]: [[0.60659855 0.00533165]\n",
      " [0.10820313 0.49978937]\n",
      " [0.34144279 0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"t_image[0]: {t_image[0]}\")\n",
    "print(f\"t_image[1]: {t_image[1]}\")\n",
    "print(f\"t_image[2]: {t_image[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e10fece-d848-4e79-b773-7b9b64600a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image2vector\n",
    "# image2vector() is the function that takes input of shape (legnth, height,3)\n",
    "# It will return the vector as (length*height*3,1)\n",
    "def image2vector(image):\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91fda65-36f7-446f-9ef5-9e5e15ae05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256906f-cd1a-4ebc-8979-f447a8af538f",
   "metadata": {},
   "source": [
    "## Normalizing Rows\n",
    "- Another important technique that we use in ML is to normalize the data.\n",
    "- It makes the performance better as the \"gradient descent converges faster after normalization\".\n",
    "-  Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c4a05-984e-4169-a73d-efe0b25b375e",
   "metadata": {
    "tags": []
   },
   "source": [
    "For example, if \n",
    " $$x = \\begin{bmatrix}\n",
    "         0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    " \\end{bmatrix}\\tag{3}$$ \n",
    " then \n",
    " $$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "     5 \\\\\n",
    "     \\sqrt{56} \\\\\n",
    " \\end{bmatrix}\\tag{4} $$\n",
    " and\n",
    " $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "     0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "     \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    " \\end{bmatrix}\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ef536-5818-411a-abdf-83691eb86eca",
   "metadata": {},
   "source": [
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting \n",
    "-  With `keepdims=True` the result will broadcast correctly against the original x.\n",
    "- `axis=1` means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set `axis=0`.\n",
    "- numpy.linalg.norm has another parameter `ord` where we specify the type of normalization to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed438be-32c5-44dc-8847-46b526165f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).\n",
    "import numpy as np\n",
    "def normalize_rows(x):\n",
    "    norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    print(\"X_norm shape: \" + str(norm.shape))\n",
    "    x = x/norm\n",
    "    print(\"X shape: \" + str(x.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f680efb9-5987-4fb7-a476-9083a393b61f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_norm shape: (2, 1)\n",
      "X shape: (2, 3)\n",
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 3., 4.],\n",
    "              [1., 6., 4.]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf8b52-921f-46df-99eb-ee08526c5fd7",
   "metadata": {},
   "source": [
    "- So you can see X shape has 3 columns but the X_norm has 1 column but still the division has been done. How? It is broadcasting, we will see it in the next topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8ed63-4d75-4272-83f6-950e8587c9ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f552b-2fd8-4dfb-91c3-e06410d95672",
   "metadata": {},
   "source": [
    "*Softmax is a mathematical function that converts a vector of real numbers into a probability distribution, where each value is between 0 and 1, and all values sum to 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccef6b-f34c-4cfd-8275-47efecab9919",
   "metadata": {},
   "source": [
    "- It is a normalization function specifically designed for multi-class classification problems.\n",
    "- In the further blogs I will explain in depth what is normalization and softmax and why we are doing them etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b763f-c031-4d5f-9f05-bef2c0311270",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  \n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c00b90-1f92-4bee-b00d-fde095e7fb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m -> represent number of training examples\n",
    "# there is a column matrix for each training example\n",
    "# And each feature will be its own row.\n",
    "\n",
    "# Softmax will be performed on every feature of every training example\n",
    "# That means it will be performed on columns\n",
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    print(\"x_exp shape: \" + str(x_exp.shape))\n",
    "    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n",
    "    print(\"x_sum shape: \" + str(x_sum.shape))\n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e6eb72-889b-4fd3-a565-3f658227b1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_exp shape: (2, 5)\n",
      "x_sum shape: (2, 1)\n",
      "t_x shape: (2, 5)\n",
      "s_x shape: (2, 5)\n",
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0 ,0]])\n",
    "s_x = softmax(t_x)\n",
    "print(\"t_x shape: \" + str(t_x.shape))\n",
    "print(\"s_x shape: \" + str(s_x.shape))\n",
    "print(\"softmax(x) = \" + str(s_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee09c84-3096-4c34-b3b0-847972a605ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "- You can see x_exp and x_sum have different shapes but we divided them and the python used broadcasting to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13475fa1-48bc-480a-ae38-3a267fb2e9b6",
   "metadata": {},
   "source": [
    "**By now we learnt the basics of python numpy and implemented some useful functions that will help you in developing machine learning or deep learning models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718cde4-54d8-4d72-b20c-2731f9f01bc1",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a894eee-8141-44b2-a1a1-c6a18abfd74e",
   "metadata": {},
   "source": [
    "Vectorization is key in deep learning because it makes your code run faster. When working with large datasets, using the right functions can save you a lot of time.\n",
    "\n",
    "Vectorization helps avoid loops and makes your code more efficient.\n",
    "\n",
    "- Lets say you are the teacher of class consitsting 30 students and you have to analyze and calculate their total marks for the final exam. And the final exam student sheet consits of 10 subjects(columns) and 30 rows(students). Now you need to add everything one by one using pen but if you use calculator it saves lot of time.\n",
    "- In the same way if you use vectotrization for the huge computations in ML/DL, you can save up lot's of time.\n",
    "- I will implement and show you how much difference you can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9ca67f3-0ccc-4e20-bd42-6983e9f14d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 70\n",
      "Time Taken: 0.08309999975608662 ms\n"
     ]
    }
   ],
   "source": [
    "# Let’s take a simple example—computing the dot product \n",
    "#of two vectors (which happens in neural networks all \n",
    "#the time).\n",
    "\n",
    "import time\n",
    "\n",
    "x1 = [1, 2, 3, 4]\n",
    "x2 = [5, 6, 7, 8]\n",
    "\n",
    "tic = time.perf_counter()\n",
    "dot_product = sum(x1[i] * x2[i] for i in range(len(x1)))\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(\"Dot Product:\", dot_product)\n",
    "print(\"Time Taken:\", (toc - tic) * 1000, \"ms\")  # Convert to milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7009b115-f39c-4630-a7c1-cfc1396e9520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 70\n",
      "Time Taken: 0.05019999935029773 ms\n"
     ]
    }
   ],
   "source": [
    "# Vectorized approach using numpy\n",
    "import time\n",
    "\n",
    "x1 = [1, 2, 3, 4]\n",
    "x2 = [5, 6, 7, 8]\n",
    "\n",
    "tic = time.perf_counter() # time.perf_counter() is more precise than time.process_time()\n",
    "dot_product = sum(x1[i] * x2[i] for i in range(len(x1)))\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(\"Dot Product:\", dot_product)\n",
    "print(\"Time Taken:\", (toc - tic) * 1000, \"ms\")  # Convert to milliseconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ae5848-6423-430f-abd6-3599893bf5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can see the difference in time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccc613-76ac-47a6-938f-143b00c01cec",
   "metadata": {},
   "source": [
    "## Numpy version of L1 Loss and L2 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f684e4a-730c-4dc1-be84-93af53919c31",
   "metadata": {},
   "source": [
    "- Loss functions measure how well a model's predictions match the actual values. Let’s implement two common loss functions using vectorization.\n",
    "**L1 loss is defined as:**\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7edcb4fc-076b-4fa7-8f29-17e29811dd09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \n",
    "    loss = np.sum(np.abs(y - yhat))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "189e2532-7ecd-424c-bd03-5beed01d1f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e072f3-d5c0-4fd8-ae1e-2337dedcec3d",
   "metadata": {},
   "source": [
    "**L2 loss is defined as:**\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "217c287f-57b3-46ec-9e38-b0ac350992b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "\n",
    "    a = y - yhat\n",
    "    loss = np.sum(np.dot(a,a))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "992c4584-ff52-496e-822a-d581b1110a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b1301-bfcb-474e-aa96-6cf0c2fea1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
